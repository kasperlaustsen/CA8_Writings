This chapter contains the controller design for the refrigeration system model developed in \cref{sec:mod}. The objective of the controller is to maintain a fixed cargo hold air temperature, while minimizing energy consumption and protecting the compressor by regulating the superheat. These objectives are not easily formulated in terms of pole placements in a simple state feedback controller. Optimal controllers are better suited in describing these objectives, as they are made to minimize a cost function, chosen by the designer. \\

An optimal control strategy is pursued for this MIMO problem. The Linear Quadratic Regulator will be implemented, which will be tested and benchmarked against the coupled PID control setup currently implemented in the Hi-Fi simulation.

After the controller is discussed, the observer structure for the control model is discussed.

\subsection{Linear Quadratic Regulator} \label{sec:linear-quadratic-regulator}
The control strategy to be implemented is the infinite-horizon Linear Quadratic Regulator, referred to as the LQR controller. The LQR problem is an optimal control problem, that finds the optimal feedback gain, that will drive the states of a dynamical system to 0. This is done by selecting a control input u, that minimizes a relevant cost function. As an example consider the system in \cref{eq:lqr_sys} with $x$ representing the system states, $u$ the control inputs and $y$ the measured outputs.\\
\begin{equation} \label{eq:lqr_sys}
	\begin{split}
		\dot{x} 	& = Ax + Bu \\
		y 	& = Cx
	\end{split}
\end{equation}

A relevant cost function for this LQR problem would be on this form:

\begin{equation} \label{eq:lqr_cost_fcn}
	J = \int_0^{\infty} \left(x^TQx + u^TRu + 2x^TNu\right)dt
\end{equation}

where
\smallskip

\begin{center}
	\begin{tabular}{l r l }
		weight & $R$         & $ > 0$ (positive definite) and symmetric       \\
		weight & $Q$ and $N$ & $\ge 0$ (positive semi-definite) and symmetric
	\end{tabular}
\end{center}
\medskip
The cross-term is typically ignored by setting $N=0$. The matrices Q and R are introduced to add weights to the various states and inputs. This allows the engineer to penalize critical states and inputs more than others. This is generally useful, especially if deviations in certain states or inputs negatively impact the system energy consumption or could cause dangerous system behavior.\\

The value of Q and R is a choice the control engineer must make. While it in practicality requires some amount of trial-and-error iteration, there are methods for choosing a reasonable starting value. An often used tuning method to obtain initial Q and R matrices is following Bryson's rule. It states that the state x$_{\textit{i}}$ should be penalized by a factor $1/\text{Max} \left(x_{\textit{i}}\right)^2$. Similarly the control input u$_{\textit{i}}$ should be penalized by a factor $\frac{1}{\text{Max} \left(u_{\textit{i}}\right)^2}$. This effectively scales the maximum value of all entries in the cost function J to 1. This is particularly useful, when the states and inputs are have numerically large differences.\\

The fact that the LQR problem aims to set the states equal to zero, may sound like a undesirable goal. If a state is representative of a temperature measured in Kelvin, driving it to 0 is likely neither a realistic nor useful goal. However, lets recall the linearization of the system performed in \cref{sec:mod_lin}. The first order Taylor expansion did not only linearize the system, it also changed the physical interpretation of the system states. Rather than being an absolute value, it is now the difference between the actual value and the equilibrium point used for linearization. This means that the LQR problem will seek to set this difference to 0, thus driving the system to the equilibrium point.

Thus the cost function with no cross-term cost becomes:

\begin{equation} \label{eq:lqr_cost_fcn}
	J = \int_0^{\infty} \left( \bar{x}^TQ\bar{x} + \bar{u}^TR\bar{u} \right)dt
\end{equation}

with $\bar{x} = x-x_o$ and $\bar{u} = u-u_o$ where $x_o$, $u_o$ are the linearisation equilibrium operating points of the states and inputs.

The optimal feedback gain K can be shown to be

\begin{equation} \label{eq:lqr_K}
	K = -R^{-1}B^{T}P
\end{equation}

where P is the unique solution to the algebraic Ricatti equation
\begin{equation} \label{eq:ricatti}
	A^TP + PA - PBR^{-1}B^TP+Q = 0
\end{equation}

While proving \cref{eq:lqr_K} explicitly is beyond the scope of this report, it is noted that it is found by following Pontryagin's minimum principle. Briefly described the process is to first define the Hamiltonian associated with the optimization problem. Differentiating the Hamiltonian with respect to the control signal u, yields the control gradient. By then setting the gradient equal to 0 and solving for u, the optimal control input is found.\\

LQR is chosen as the initial control strategy due to its intuitive way of use, as well the many proven properties. These include a guaranteed 6dB gain- and $60^\circ$ phase margin \cite{Doyle}.

%\subsubsection{Integral action}
%In most control problems it is the objective to keep an output at a reference value. While there are many methods of achiving this, the most commonly used are reference scaling and integral action. Reference scaling involves inserting a gain in the forward path of the reference signal. The value of the gain is the reciprocal of the system DC gain, resulting in the output settling at the value of the reference. This method requires accurate knowledge of the system to calculate the DC gain, and is thus very sensitive to variations. Integral action is far more robust. The common way to include integral action in state space controllers is to extend the state vector, with one or more artificial integral states.
%
%\begin{equation}
%	\tilde{x} = \begin{bmatrix}
%		x \\
%		x_i
%	\end{bmatrix}
%\end{equation}
%
%where
%
%\begin{equation}
%	x_i = \int_{0}^{t}(r-y)dt
%\end{equation}
%
%The method of introducing an integral state poses a conflicting objective for the LQR algorithm, as it will attempt to set \textit{all} states equal to zero. Let us recall that y is a linear combination of the states: $y = Cx$. Since $x$ is already being driven to zero, it is only sensible to drive $y=r$ if $r=0$.\\
%
%
%
%Instead one must perform a state transformation to a coordinate system, where $x=0$ satisfies $y=r$. Here, we again reap the benefits of the linearisation performed when obtaining the system model. In this linearisation the origin of the state space was moved from 0 to an operating point. This means if the system is internally stable, it will settle at $x=0 \rightarrow y=r$. At this point the control inputs are also zero, but since these are in fact $u-u_o$, it means the operating point control signal is fed to the plant. \\
%
%Since the system will naturally settle at $y=r$, we have eliminitated the need for an integral state by making a proper linearisation. The downside to this strategy is that the reference is locked in by the choice of operating point. Selecting a new reference value will require a new linearisation at the a operating point associated with that reference value - not a trivial task.

\subsubsection{LQR controller gain}
As described in \cref{sec:linear-quadratic-regulator}, in order to find the optimal controller gain, the weight matrices Q and R needs to be determined. They are chosen as:

\begin{equation}
	Q =
	\left(\begin{array}{cccccccc}
		1 & 0 & 0 & 0 & 0 & 0 & 0 & 0  \\
		0 & 1 & 0 & 0 & 0 & 0 & 0 & 0  \\
		0 & 0 & 1 & 0 & 0 & 0 & 0 & 0  \\
		0 & 0 & 0 & 1 & 0 & 0 & 0 & 0  \\
		0 & 0 & 0 & 0 & 2 & 0 & 0 & 0  \\
		0 & 0 & 0 & 0 & 0 & 1 & 0 & 0  \\
		0 & 0 & 0 & 0 & 0 & 0 & 2 & 0  \\
		0 & 0 & 0 & 0 & 0 & 0 & 0 & 1
	\end{array}\right)
	\text{  and  }
	R =
		\left(\begin{array}{cc}
		1 & 0  \\
		0 & 0.025
	\end{array}\right)
\end{equation}

\medskip

Using Q and R as identitity matrices of appropriate dimensions the following optimal feedback gain is calculated:

\begin{equation}
	K =
	\left(\begin{array}{cccccccccc}
		-0.0784 &  -0.0140 &  -0.0203 &   1.1457 &  -0.1743 &  -0.1021 &  -0.6274 &   0.0049 \\
		0.5679 &   0.0786 &   0.1447 &  -4.0389 &   0.7489 &   0.4246 &   2.5119 &   0.0041
	\end{array}\right)
\end{equation}

\subsection{Observer Gain} \label{sec:observer-gain}
As stated previously, the control model is to be used as an observer for the actual system. By measuring the inputs and outputs the control model will produce an estimate of the states. These observed states will be used for feedback control. The observer structure used in this project is seen in \cref{fig:luenberger}.

The difference in the plant and model outputs are multiplied by a observer gain L, and added to the observed state derivatives. This is known as the Luenberger observer. The error $e$ is defined as
\begin{equation} \label{eq:error}
	e = x-\hat{x}
\end{equation}

subject to the dynamics
\begin{equation}
	\dot{e} = \dot{x}-\dot{\hat{x}}
\end{equation}

$\dot{x}$ and $\dot{\hat{x}}$ can be written as

\begin{equation}
	\dot{x} = A + Bu
\end{equation}


\begin{equation}
	\dot{\hat{x}} = A\hat{x} + LCe + Bu
\end{equation}

Substituting these into \cref{eq:error} yields

\begin{equation}
	\dot{e} = Ax + Bu - ( A\hat{x} + LCe  + Bu)
\end{equation}

\begin{equation}
	\dot{e} = Ae - LCe
\end{equation}

\begin{equation} \label{eq:obs_error_dot}
	\dot{e} = (A - LC)e
\end{equation}

\begin{figure}[h!]
	\centering
	\includegraphics[width=0.9\textwidth]{Graphics/Observer_Diagram.pdf}
	\caption{Block diagram of observer structure}
	\label{fig:luenberger}
\end{figure}


This observer structure guerantees that if $A-LC$ is stable, the error $e$ will tend to zero as time goes to infinity. This is under the assumption that the pair $A$ and $C$ are observable. This is a given, since a Kalman decomposition of the system has been performed. The eigenvalues of $A-LC$ must be negative to ensure stability. Further, the observer must be faster than the dynamics of the plant, so rapid changes in the states can be observed. A practical approach is to choose the eigenvalues of $A-LC$ 5-10 times larger than the eigenvalues of $A-BK$. This method is used for the observer gain is this project, and thus the value of L is calculated following the controller design subsection.
\todo[inline]{FIX KOMMENTARER FRA JAN OG JOHN IFT DET HER SHIT}